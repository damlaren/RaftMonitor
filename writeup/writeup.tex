
\documentclass[UTF8]{article}

\usepackage{times}
\usepackage{graphicx} % more modern
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{qtree}
\usepackage{float}
\epstopdfsetup{outdir=./}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[accepted]{icml2014}
\icmltitlerunning{TITLE}
\pagenumbering{alph}

\usepackage{enumitem}
\setlist[itemize]{noitemsep}

\begin{document} 

\twocolumn[

\icmltitle{TITLE}

\icmlauthor{David McLaren}{dmclaren@stanford.edu}
\icmlauthor{Elizabeth Walkup}{ewalkup@stanford.edu}
\icmlauthor{Patrick Harvey}{p1harvey@stanford.edu}
\icmladdress{Computer Science Department, Stanford University}

\icmlkeywords{cs244b, Raft, distributed systems}

\vskip 0.3in
]
\setlength\parindent{24pt}

\begin{abstract}
	
ABSTRACT

\end{abstract} 

\section{Introduction}

basic description of system (maybe 1/2 page)

\section{Simulating Partitions}

concepts for simulating partitions, sniffing packets
- iptables and packet redirection
- sniffing with libtins

\section{System Overview}

The checker is composed of various modules which are held together by a test driver program written in C++. Base classes representing Raft clusters and clients are adapted for different Raft implementations to support reuse of functionality and enable the same tests to be executed across different platforms. The test driver proceeds through several stages.

\subsection{Setup}

Simulating a cluster makes use of generated, virtual network interfaces to allow RAFT cluster nodes to appear to one another as though on independent devices. Much of this setup is independent of the specific RAFT implementation; it is executed via shell script.

Once environment setup is complete, the test driver begins by starting a new Raft cluster with a specified number of nodes, and clients to request operations on the cluster. The driver calls the client interface to request operations on the cluster.

\subsection{Client API}

Different implementations of RAFT may have a different interface for receiving and responding to client requests, so the implementation of tests that include client requests must include functionality specific to individual RAFT implementations. To avoid large amounts of redundant functionality and allow the checker to be applied to new RAFT implementations more easily, a Client API is provided. The client class provides a common interface to connect to a Raft cluster and read or write files on it. Each client subclass supports these operations for a different Raft implementation.

The API is defined as RaftClient and RaftClusterConfig base classes in C++ with several pure virtual methods, which must be extended to provide a client for a particular RAFT implementation. This client can then be passed to the RAFT checker's tests and manipulated via the API in a manner externally identical to any other client. The RaftClusterConfig implementation implements implementation-specific logic to set up a RAFT cluster, and the RaftClient implements logic to handle a client capable of communicating with that cluster. RAFT implementations that provide a much cleaner interface for clients in some language other than C++ can be supported by creating a C++ client implementation that calls out to execute functionality in scripts or programs of the language in question.

The RaftClusterConfig API provides helper functions to create or kill subprocesses (to take down nodes in the Raft cluster). It requires subclasses to implement the following methods for different Raft implementations:

\begin{itemize}
\item \texttt{launchCluster(int numNodes, int port)}: Start a new Raft cluster with the requested number of nodes, listening on the specified port.
\item \texttt{stopCluster(int numNodes)}: Shut down Raft cluster with the provided number of nodes.
\end{itemize}

The Client API requires that RaftClients implement the following methods:

\begin{itemize}
\item \texttt{createClient(RaftClusterConfig* config)}: Create and initialize a client.
\item \texttt{destroyClient()}: Stop a client and free any resources it uses.
\item \texttt{connectToCluster(string hosts)}: Connect client to a Raft cluster consisting of the specified hosts.
\item \texttt{writeFile(string path, string contents)}: Write a file with content value on cluster at specified path.
\item \texttt{readFile(string path)}: Read file contents at path and return as a string.
\end{itemize}

\subsection{RaftMonitor}

The RaftMonitor monitors and responds to messages passed internally among the RAFT cluster's nodes by acting as a proxy or middleman to all of the cluster's internal communications. The IP address and port presented to the rest of the cluster for a RAFT node is instead one assigned to the monitor. However, on setup this should be completely transparent to the RAFT cluster, since the default behavior of the monitor is just to log packets, rewrite their destination to the appropriate node based on the address and/or port the monitor received the packet on, and resend the packet. Thus when non-invasive testing is being performed, the cluster should operate in an entirely normal manner unaffected by the monitor's presence.

For this to be the case it is important that any processing the monitor performs on the packet not lengthen too much the time required for the packet to reach the RAFT node it is intended for, lest the monitor interfere with RAFT's heartbeat messages and thus affect the cluster's operation. However, this does seem a manageable constraint: since the checking program simulates a cluster within a single machine, round-trip times for packet transmission are low (on the order of $0.05$-$0.2$ milliseconds). Recommended settings for RAFT election timeout are in the low hundreds of milliseconds \cite{raftPaper}. Simple logging, mangling, and resending of packets, then, is unlikely to cause an unexpected timeout under this sort of mode of operation. However, more expensive actions such as killing one of the RAFT node processes could be concerning given this time constraint; actions that are too expensive are placed in a separate thread. For example, the action to kill a node would consist of creating a separate thread tasked with killing the node's RAFT process, and the monitor will begin dropping all packets to and from that node in the meantime.

\subsection{Future Work}

- plans for additional checkers for more raft implementations
- more functionality in monitors? delaying or blocking packets

\section{Tests}

discussion of tests supported by the system:
- basic sanity check (client does op expected to succeed)
- multiple clients writing to file system
- failure and restart of raft nodes
- partitions

\subsection{External Tests}
External tests examine the results of requests made of the RAFT cluster; fundamentally for correctness they are limited to examining a cluster only as a normal client of it could, although the monitor's message logs and statistics can still be considered.

\subsection{Invasive Tests}
Invasive tests rely on the RaftMonitor's position as a middleman in the internal communications between the RAFT nodes. The monitor allows the dropping of particular packets or closing of connections, as well as the killing or restarting of nodes at more specific points in execution than can be done externally, such as causing a node to die after having received a request but before it replies.

\subsection{Future Work}

additional tests we plan to do or didn't get to


\bibliographystyle{icml2014}
\bibliography{citations}


\end{document} 

